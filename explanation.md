# BASE IMAGE CHOICE WHEN BUILDING CONTAINERS

What is in a base container image is still an operating system that derives its security, reliability, and life cycle from the underlying operating system on which it is built. Compatibility should also be considered. because it was a Node.js app ,I used node as my base image for both web and API.

# DOCKERFILE DIRECTIVES USED IN THE CREATION AND RUNNING OF EACH CONTAINER.

# Client Service Dockerfile

FROM node:13.12.0 WORKDIR /app COPY package*.json ./ RUN npm install COPY . . EXPOSE 3000 CMD ["npm", "start"]

    FROM node:13.12.0-alpine , I'm using the node:13.12.0-alpine image,because it is a light weight image(small image size).

    WORKDIR /app - used to define the working directory of a Docker container at any given time.

    COPY package*.json ./ - Copies the package.json and package-lock.json files from the local directory to the Docker image,gives a clear description of all packages your app needs to run.

    RUN npm install - Runs the npm install command inside the Docker image ,It is a package manager for node, written in Javascript. We can install any required packages with the npm  install command.

    COPY . . - Copies the entire local directory to the Docker image.

    EXPOSE 3000 - Informs Docker that the container will listen on port 3000.

    CMD ["npm", "start"] - Specifies the command to run when the container starts. In this case, it runs the npm start command to start the web server.

# Api Service Dockerfile

    FROM node:alpine
    WORKDIR /app
    COPY package*.json ./
    RUN npm install
    COPY . .
    EXPOSE 5000
    CMD [ "npm", "run","start" ]

The same directives as client service dockerfile
# DOCKER-COMPOSE NETWORKING (Application port allocation and a bridge network implementation) where necessary.

In docker-compose.yml file, I specified a bridge network called yolo-my-net for my microservices to communicate with each other: I specified network driver to type bridge.

  # GIT WORKFLOW .
    Used centralised git workflow .

    Forking and cloning yolo repository to my local machine using git clone (for cloning)


    Create a Dockerfile for each microservice. I created a Dockerfile for each microservice in its respective branch. I followed best practices for writing Dockerfiles, such as using a lightweight base image and only including necessary dependencies.

    Build and test each Docker image locally. I used the docker build command to build each Docker image locally, and then used the docker run command to test each image to make sure it was running correctly.before pushing to github.

    Create a docker-compose.yml file. I created a docker-compose.yml file in the main branch to orchestrate the containers for each microservice. I specified the Docker image for each service and defined the networking

    Test the Docker Compose setup locally. I used the docker-compose up command to start the Docker Compose setup locally and tested that the microservices were able to communicate with each other.

    Push the changes to the repository. Once I was satisfied that everything was working correctly, I committed and pushed the changes to the repository. using quality and descriptive commits.

    

# Successful running of the applications and if not, debugging measures applied.
I updated the package.json file to resolve the error cannot find module dotenv.

# Good practices such as Docker image tag naming standards for ease of identification of images and containers.
        I used the repository name together with the service name to tag my images and versioning method to name my images  yoloclient:v1.0.1 yolobackend:v1.0.1 2.I used lowercase letters to do this

# Docker-compose volume definition and usage (where necessary).
 I created a volume called yolovol ,  volumes are used for persisting data generated by and used by Docker containers.

# Ansible Playbook for Deploying YOLO Application

This is an Ansible playbook for deploying the YOLO application on both the frontend and backend servers. The YOLO application is a web-based chat application that allows users to create chat rooms and chat with each other in real-time.
# Server Setup

The playbook assumes that the servers have already been provisioned and are running Ubuntu. It also assumes that Ansible is installed on the control machine and that SSH access to the servers has been set up.
# Playbook Overview

The playbook consists of two roles: "client side" and "backend app". The "client side" role installs and sets up the frontend server, while the "backend app" role installs and sets up the backend server.
# Client Side Role

The "client side" role consists of the following tasks:

    Install GPG
    Install the GPG key for Node.js LTS
    Install the Node.js LTS repos
    Update apt packages
    Install Node.js
    Clone the YOLO repository
    Install dependencies from lockfile
    Start the YOLO application

The role installs Node.js on the frontend server and clones the YOLO repository from GitHub. It also installs the dependencies required to run the application and starts the application.
# Backend App Role

The "backend app" role consists of the following tasks:

    Install GPG
    Install the GPG key for Node.js LTS
    Install the Node.js LTS repos
    Update apt packages
    Install Node.js
    Clone the YOLO repository
    Install dependencies from lockfile
    Start the YOLO application

The role installs Node.js on the backend server and clones the YOLO repository from GitHub. It also installs the dependencies required to run the application and starts the application.
# Variables

The playbook uses the following variable:

    NODEJS_VERSION: specifies the version of Node.js to install. In this playbook, it is set to "14".

# Tags

Tags are used to group tasks together and allow for selective execution of specific tasks. The playbook uses the following tags:

    nodejs: used for tasks related to installing Node.js.
    install: used for tasks related to installing software packages.
    setup: used for tasks related to setting up the servers.
    app: used for tasks related to installing the YOLO application.
    build: used for tasks related to building the application.
    deploy: used for tasks related to deploying the application.

# Conclusion

This Ansible playbook provides an automated way to deploy the YOLO application on both the frontend and backend servers. By using roles, variables, and tags, the playbook can be easily customized and reused for other applications.

# Explanation of Frontend Deployment

This YAML file is a Kubernetes deployment configuration for the frontend application of the YOLO project. The deployment consists of a single replica of a container running the YOLO frontend application.

# Metadata

The metadata section of this file contains information about the deployment such as its name, labels, and annotations. The labels and annotations are used to provide additional information about the deployment.

# Spec

The spec section of this file defines the desired state of the deployment. It specifies that there should be one replica of the frontend container running, and that the container should be created from the specified image.

The deployment selector matches labels that are assigned to pods in the cluster. The labels in this file are used to match the frontend pod.

The template section of the spec defines the pod template for the deployment. It specifies the labels that will be assigned to the pod and the container that should be run in the pod.

The frontend container runs the YOLO frontend application and exposes port 3000. The container also sets two environment variables, CLIENT_HOST and CLIENT_PORT, which are used to connect to the backend service.

# Explanation of YAML code for backend deployment and persistent volume claim
Statefulset and Deployment


# Metadata

    name: This is the name of the deployment.
    labels: These are custom labels that can be used to identify and group Kubernetes objects.

# Selector

The selector specifies how the Deployment finds which Pods to manage.
# Replicas

The replicas field specifies the desired number of replicas for the Deployment.
# Template

The template field specifies the Pod template to use for the Deployment.
# Metadata

    labels: These are the labels applied to the Pod.

# Spec

The spec field of the Pod template specifies the container(s) to run in the Pod.
# Containers

    name: The name of the container.
    image: The Docker image to use for the container.
    ports: The ports to expose for the container.
    volumeMounts: The volumes to mount in the container.

# Env

    name: The name of the environment variable.
    valueFrom.secretKeyRef.name: The name of the Secret that contains the environment variable value.
    valueFrom.secretKeyRef.key: The key in the Secret that contains the environment variable value.

# Volumes

    name: The name of the volume.
    persistentVolumeClaim.claimName: The name of the PersistentVolumeClaim to use for the volume.

# Persistent Volume Claim

The Persistent Volume Claim (PVC) is a separate YAML object that specifies the storage requirements for the Deployment.
# Metadata

    name: The name of the PVC.

# Spec

The spec field of the PVC specifies the access mode and storage requirements.

    accessModes: The access mode for the PVC. In this case, it's ReadWriteOnce, meaning the volume can be mounted as read-write by a single node.
    resources.requests.storage: The requested storage size for the PVC.
    storageClassName: The name of the StorageClass to use for the PVC. In this case, it's set to standard.


# Explanation of Backend Service

This YAML file is a Kubernetes service configuration for the backend application of the YOLO project. The service provides a stable IP address and DNS name for the backend deployment.

# Metadata

The metadata section of this file contains information about the service such as its name.
# Spec

The spec section of this file defines the desired state of the service. It specifies that the service should be of type LoadBalancer and that it should match the labels of the backend deployment. The ports section specifies the port mappings between the service and the backend deployment.
# Explanation of Frontend Service

This YAML file is a Kubernetes service configuration for the frontend application of the YOLO project. The service provides a stable IP address and DNS name for the frontend deployment.
# Metadata

The metadata section of this file contains information about the service such as its name, labels, and annotations. The labels and annotations are used to provide additional information about the service.
# Spec

The spec section of this file defines the desired state of the service. It specifies that the service should be of type LoadBalancer and that it should match the labels of the frontend deployment. The ports section specifies the port mappings between the service and the frontend deployment.

I hope this explanation helps you understand the purpose and configuration of each file in your deployment.

# THE WEBSITE URL : http://35.228.80.245:3000